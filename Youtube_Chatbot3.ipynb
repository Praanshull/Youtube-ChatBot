{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85220fb3-5afa-4c52-8e93-c34f90d7e04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain-community faiss-cpu tiktoken python-dotenv\n",
    "!pip install -q langchain langchain_huggingface langchain_core\n",
    "!pip install -q langchain-text-splitters\n",
    "!pip install -q youtube-transcript-api\n",
    "!pip install -q ragas  # For evaluation\n",
    "!pip install -q rank-bm25  # For BM25 keyword search\n",
    "!pip install -q sentence-transformers  # For reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a9572c6-0486-4689-9a77-d427f9b1f1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import numpy as np\n",
    "\n",
    "# YouTube & Text Processing\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from youtube_transcript_api._errors import TranscriptsDisabled\n",
    "\n",
    "# LangChain Core\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import (\n",
    "    RunnablePassthrough, \n",
    "    RunnableLambda, \n",
    "    RunnableParallel,\n",
    "    RunnableBranch\n",
    ")\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# LangChain - Hugging Face\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint, ChatHuggingFace\n",
    "\n",
    "# BM25 for Hybrid Search\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Sentence Transformers for Reranking\n",
    "from sentence_transformers import CrossEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "310b7fb3-1a97-4df8-b122-5964f8e89ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"Centralized configuration\"\"\"\n",
    "    # API Keys\n",
    "    HF_TOKEN = \"your_token_here\"\n",
    "\n",
    "    \n",
    "    # Video\n",
    "    VIDEO_ID = \"kCc8FmEb1nY\"\n",
    "    \n",
    "    # Chunking\n",
    "    CHUNK_SIZE = 1000\n",
    "    CHUNK_OVERLAP = 200\n",
    "    \n",
    "    # Retrieval\n",
    "    RETRIEVAL_K = 10\n",
    "    FINAL_K = 4\n",
    "    ALPHA = 0.5  # Weight for hybrid search (0.5 = equal weight)\n",
    "    \n",
    "    # Models\n",
    "    EMBEDDING_MODEL = \"BAAI/bge-base-en-v1.5\"\n",
    "    LLM_MODEL = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "    RERANKER_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "    \n",
    "    # Memory\n",
    "    MAX_HISTORY = 5\n",
    "\n",
    "os.environ[\"HUGGING_FACE_ACCESS_TOKEN\"] = Config.HF_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9deae3dd-8375-488e-9223-3ac0782a27f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationMemory:\n",
    "    \"\"\"Simple conversation memory with history management\"\"\"\n",
    "    \n",
    "    def __init__(self, max_history: int = Config.MAX_HISTORY):\n",
    "        self.max_history = max_history\n",
    "        self.history = []\n",
    "    \n",
    "    def add_interaction(self, question: str, answer: str):\n",
    "        \"\"\"Add Q&A pair to memory\"\"\"\n",
    "        self.history.append({\"question\": question, \"answer\": answer})\n",
    "        if len(self.history) > self.max_history:\n",
    "            self.history = self.history[-self.max_history:]\n",
    "    \n",
    "    def get_context(self) -> str:\n",
    "        \"\"\"Format history for prompt\"\"\"\n",
    "        if not self.history:\n",
    "            return \"No previous conversation.\"\n",
    "        \n",
    "        context = \"Previous conversation:\\n\"\n",
    "        for i, interaction in enumerate(self.history, 1):\n",
    "            context += f\"Q{i}: {interaction['question']}\\n\"\n",
    "            context += f\"A{i}: {interaction['answer']}\\n\\n\"\n",
    "        return context\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear history\"\"\"\n",
    "        self.history = []\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9105ba9d-7bc7-4052-a6d7-8a7bb26d308e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_youtube_transcript(video_id: str) -> Tuple[str, dict]:\n",
    "    \"\"\"Fetch transcript from YouTube video\"\"\"\n",
    "    try:\n",
    "        fetched_transcript = YouTubeTranscriptApi().fetch(video_id, languages=['en'])\n",
    "        transcript_obj = fetched_transcript.to_raw_data()\n",
    "        transcript = \" \".join([entry[\"text\"] for entry in transcript_obj])\n",
    "        \n",
    "        metadata = {\n",
    "            \"video_id\": video_id,\n",
    "            \"url\": f\"https://www.youtube.com/watch?v={video_id}\",\n",
    "            \"length\": len(transcript),\n",
    "            \"num_segments\": len(transcript_obj)\n",
    "        }\n",
    "        \n",
    "        return transcript, metadata\n",
    "    \n",
    "    except TranscriptsDisabled:\n",
    "        raise Exception(\"âš ï¸ No captions available for this video\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"âš ï¸ Error fetching transcript: {e}\")\n",
    "\n",
    "\n",
    "def chunk_transcript(transcript: str, metadata: dict) -> List[Document]:\n",
    "    \"\"\"Split transcript into chunks with metadata\"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=Config.CHUNK_SIZE,\n",
    "        chunk_overlap=Config.CHUNK_OVERLAP,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    chunks = splitter.create_documents(\n",
    "        texts=[transcript],\n",
    "        metadatas=[metadata]\n",
    "    )\n",
    "    \n",
    "    # Add chunk IDs\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk.metadata[\"chunk_id\"] = i\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ddd22c7-8655-4273-84ad-a01c05b769db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridRetriever:\n",
    "    \"\"\"Hybrid retrieval combining FAISS and BM25 with reranking\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        vector_store: FAISS,\n",
    "        bm25_index: BM25Okapi,\n",
    "        chunks: List[Document],\n",
    "        reranker: CrossEncoder,\n",
    "        k: int = Config.RETRIEVAL_K,\n",
    "        final_k: int = Config.FINAL_K,\n",
    "        alpha: float = Config.ALPHA\n",
    "    ):\n",
    "        self.vector_store = vector_store\n",
    "        self.bm25_index = bm25_index\n",
    "        self.chunks = chunks\n",
    "        self.reranker = reranker\n",
    "        self.k = k\n",
    "        self.final_k = final_k\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def retrieve(self, query: str) -> List[Document]:\n",
    "        \"\"\"Execute hybrid retrieval with reranking\"\"\"\n",
    "        \n",
    "        # 1. Semantic Search (FAISS)\n",
    "        semantic_results = self.vector_store.similarity_search_with_score(query, k=self.k)\n",
    "        \n",
    "        # 2. Keyword Search (BM25)\n",
    "        tokenized_query = query.lower().split()\n",
    "        bm25_scores = self.bm25_index.get_scores(tokenized_query)\n",
    "        bm25_top_indices = np.argsort(bm25_scores)[-self.k:][::-1]\n",
    "        \n",
    "        # 3. Combine Results\n",
    "        combined_scores = {}\n",
    "        \n",
    "        # Add semantic results\n",
    "        semantic_scores = [score for _, score in semantic_results]\n",
    "        max_semantic = max(semantic_scores) if semantic_scores else 1\n",
    "        \n",
    "        for doc, score in semantic_results:\n",
    "            doc_id = id(doc)\n",
    "            normalized_score = score / max_semantic\n",
    "            combined_scores[doc_id] = {\n",
    "                'doc': doc,\n",
    "                'score': self.alpha * normalized_score\n",
    "            }\n",
    "        \n",
    "        # Add BM25 results\n",
    "        max_bm25 = max(bm25_scores) if max(bm25_scores) > 0 else 1\n",
    "        \n",
    "        for idx in bm25_top_indices:\n",
    "            doc = self.chunks[idx]\n",
    "            doc_id = id(doc)\n",
    "            normalized_score = bm25_scores[idx] / max_bm25\n",
    "            \n",
    "            if doc_id in combined_scores:\n",
    "                combined_scores[doc_id]['score'] += (1 - self.alpha) * normalized_score\n",
    "            else:\n",
    "                combined_scores[doc_id] = {\n",
    "                    'doc': doc,\n",
    "                    'score': (1 - self.alpha) * normalized_score\n",
    "                }\n",
    "        \n",
    "        # Sort by combined score\n",
    "        sorted_results = sorted(\n",
    "            combined_scores.values(),\n",
    "            key=lambda x: x['score'],\n",
    "            reverse=True\n",
    "        )[:self.k]\n",
    "        \n",
    "        # 4. Reranking\n",
    "        candidates = [item['doc'] for item in sorted_results]\n",
    "        pairs = [[query, doc.page_content] for doc in candidates]\n",
    "        rerank_scores = self.reranker.predict(pairs)\n",
    "        \n",
    "        # Get top final_k after reranking\n",
    "        reranked_indices = np.argsort(rerank_scores)[-self.final_k:][::-1]\n",
    "        final_docs = [candidates[i] for i in reranked_indices]\n",
    "        \n",
    "        return final_docs\n",
    "\n",
    "\n",
    "class MultiQueryRetriever:\n",
    "    \"\"\"Multi-query retrieval for better coverage\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        base_retriever: HybridRetriever,\n",
    "        llm: Any,\n",
    "        num_queries: int = 3\n",
    "    ):\n",
    "        self.base_retriever = base_retriever\n",
    "        self.llm = llm\n",
    "        self.num_queries = num_queries\n",
    "        \n",
    "        # Multi-query prompt\n",
    "        self.multi_query_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an AI assistant that generates multiple search queries.\n",
    "\n",
    "Given a user question, generate 3 different versions of the question to retrieve relevant documents.\n",
    "Provide these alternative questions separated by newlines.\n",
    "\n",
    "Original question: {question}\n",
    "\n",
    "Alternative questions:\n",
    "\"\"\")\n",
    "    \n",
    "    def generate_queries(self, question: str) -> List[str]:\n",
    "        \"\"\"Generate multiple query variations\"\"\"\n",
    "        try:\n",
    "            prompt = self.multi_query_prompt.invoke({\"question\": question})\n",
    "            response = self.llm.invoke(prompt)\n",
    "            queries = [q.strip() for q in response.content.split('\\n') if q.strip()]\n",
    "            return [question] + queries[:self.num_queries-1]\n",
    "        except:\n",
    "            return [question]\n",
    "    \n",
    "    def retrieve(self, question: str) -> Tuple[List[Document], List[str]]:\n",
    "        \"\"\"Retrieve using multiple query variations and return both docs and queries\"\"\"\n",
    "        queries = self.generate_queries(question)\n",
    "        \n",
    "        # Retrieve for each query\n",
    "        all_docs = []\n",
    "        seen_content = set()\n",
    "        \n",
    "        for query in queries:\n",
    "            docs = self.base_retriever.retrieve(query)\n",
    "            \n",
    "            # Deduplicate\n",
    "            for doc in docs:\n",
    "                content_hash = hash(doc.page_content)\n",
    "                if content_hash not in seen_content:\n",
    "                    seen_content.add(content_hash)\n",
    "                    all_docs.append(doc)\n",
    "        \n",
    "        # Final reranking if we have too many\n",
    "        if len(all_docs) > Config.FINAL_K:\n",
    "            pairs = [[question, doc.page_content] for doc in all_docs]\n",
    "            rerank_scores = self.base_retriever.reranker.predict(pairs)\n",
    "            reranked_indices = np.argsort(rerank_scores)[-Config.FINAL_K:][::-1]\n",
    "            final_docs = [all_docs[i] for i in reranked_indices]\n",
    "        else:\n",
    "            final_docs = all_docs\n",
    "        \n",
    "        return final_docs, queries  # Return both docs and queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f11c22f4-82bc-4d3c-bfe3-bb2146c6940d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs: List[Document]) -> str:\n",
    "    \"\"\"Format documents for context\"\"\"\n",
    "    return \"\\n\\n---\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "\n",
    "def create_rag_chain(\n",
    "    retriever: MultiQueryRetriever,\n",
    "    llm: Any,\n",
    "    memory: ConversationMemory\n",
    "):\n",
    "    \"\"\"\n",
    "    Create the main RAG chain with all components\n",
    "    \n",
    "    Chain Flow:\n",
    "    1. Input: question\n",
    "    2. Parallel: retrieve contexts + get memory\n",
    "    3. Format context\n",
    "    4. Build prompt with context + memory + question\n",
    "    5. Generate answer\n",
    "    6. Store in memory\n",
    "    7. Return answer + contexts + queries\n",
    "    \"\"\"\n",
    "    \n",
    "    # RAG Prompt Template\n",
    "    rag_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a helpful AI assistant answering questions about a YouTube video.\n",
    "\n",
    "{conversation_history}\n",
    "\n",
    "Use the following context from the video to answer the question.\n",
    "If the context doesn't contain the answer, say you don't know.\n",
    "Be concise but informative.\n",
    "\n",
    "Context from video:\n",
    "{context}\n",
    "\n",
    "Current question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\")\n",
    "    \n",
    "    # Define retrieval runnable that returns both docs and queries\n",
    "    def retrieve_with_queries(q):\n",
    "        docs, queries = retriever.retrieve(q)\n",
    "        return {\"docs\": docs, \"queries\": queries}\n",
    "    \n",
    "    retrieval_runnable = RunnableLambda(retrieve_with_queries)\n",
    "    \n",
    "    # Define memory runnable\n",
    "    memory_runnable = RunnableLambda(lambda _: memory.get_context())\n",
    "    \n",
    "    # Main chain\n",
    "    chain = (\n",
    "        # Step 1: Parallel retrieval and memory\n",
    "        RunnableParallel({\n",
    "            \"question\": RunnablePassthrough(),\n",
    "            \"retrieval_result\": retrieval_runnable,\n",
    "            \"conversation_history\": memory_runnable\n",
    "        })\n",
    "        # Step 2: Extract and format contexts\n",
    "        .assign(\n",
    "            contexts=lambda x: x[\"retrieval_result\"][\"docs\"],\n",
    "            queries=lambda x: x[\"retrieval_result\"][\"queries\"],\n",
    "            context=lambda x: format_docs(x[\"retrieval_result\"][\"docs\"])\n",
    "        )\n",
    "        # Step 3: Generate answer\n",
    "        .assign(\n",
    "            answer=rag_prompt | llm | StrOutputParser()\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return chain\n",
    "\n",
    "\n",
    "def create_chat_function(chain: Any, memory: ConversationMemory):\n",
    "    \"\"\"\n",
    "    Wrap chain with memory storage and return answer, contexts, and queries\n",
    "    \"\"\"\n",
    "    \n",
    "    def chat(question: str, verbose: bool = True) -> Tuple[str, List[Document], List[str]]:\n",
    "        \"\"\"\n",
    "        Chat function that handles memory and returns answer + contexts + queries\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            verbose: Print processing info\n",
    "            \n",
    "        Returns:\n",
    "            (answer, retrieved_contexts, generated_queries)\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"â“ Question: {question}\")\n",
    "            print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        try:\n",
    "            # Run chain\n",
    "            result = chain.invoke(question)\n",
    "            \n",
    "            answer = result[\"answer\"].strip()\n",
    "            contexts = result[\"contexts\"]\n",
    "            queries = result[\"queries\"]\n",
    "            \n",
    "            # Store in memory\n",
    "            memory.add_interaction(question, answer)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"ğŸ” Generated Queries:\")\n",
    "                for i, q in enumerate(queries, 1):\n",
    "                    print(f\"   {i}. {q}\")\n",
    "                print(f\"\\nğŸ¤– Answer: {answer}\")\n",
    "                print(f\"\\nğŸ’¡ (Retrieved {len(contexts)} relevant chunks)\")\n",
    "                print(f\"{'='*80}\\n\")\n",
    "            \n",
    "            return answer, contexts, queries\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\nâš ï¸ Error: {e}\\n\")\n",
    "            raise\n",
    "    \n",
    "    return chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d020e97a-5276-4b25-ba75-546cf207bd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_chat(chat_function: Any, memory: ConversationMemory, video_url: str):\n",
    "    \"\"\"Create interactive chat loop\"\"\"\n",
    "    \n",
    "    def interactive_chat():\n",
    "        \"\"\"\n",
    "        Interactive chatbot session\n",
    "        \n",
    "        Commands:\n",
    "        - 'quit' or 'exit': End chat\n",
    "        - 'clear': Clear conversation memory\n",
    "        - 'history': Show conversation history\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ğŸ’¬ YOUTUBE VIDEO CHATBOT - Interactive Mode\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"\\nğŸ”¹ Video: {video_url}\")\n",
    "        print(\"\\nğŸ® Commands:\")\n",
    "        print(\"   - Type your question to chat\")\n",
    "        print(\"   - 'quit' or 'exit': End chat\")\n",
    "        print(\"   - 'clear': Clear conversation history\")\n",
    "        print(\"   - 'history': Show conversation history\")\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "        \n",
    "        while True:\n",
    "            user_input = input(\"You: \").strip()\n",
    "            \n",
    "            if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "                print(\"\\nğŸ‘‹ Goodbye! Thanks for chatting!\\n\")\n",
    "                break\n",
    "            \n",
    "            elif user_input.lower() == 'clear':\n",
    "                memory.clear()\n",
    "                print(\"ğŸ§¹ Conversation history cleared!\\n\")\n",
    "                continue\n",
    "            \n",
    "            elif user_input.lower() == 'history':\n",
    "                print(\"\\nğŸ“œ Conversation History:\")\n",
    "                print(\"=\"*60)\n",
    "                if len(memory) == 0:\n",
    "                    print(\"(No conversation history yet)\")\n",
    "                else:\n",
    "                    print(memory.get_context())\n",
    "                print(\"=\"*60 + \"\\n\")\n",
    "                continue\n",
    "            \n",
    "            elif not user_input:\n",
    "                continue\n",
    "            \n",
    "            # Get answer\n",
    "            try:\n",
    "                answer, docs, queries = chat_function(user_input, verbose=False)\n",
    "                \n",
    "                print(f\"\\nğŸ” Generated Queries:\")\n",
    "                for i, q in enumerate(queries, 1):\n",
    "                    print(f\"   {i}. {q}\")\n",
    "                \n",
    "                print(f\"\\nğŸ¤– Bot: {answer}\")\n",
    "                print(f\"\\nğŸ’¡ (Retrieved {len(docs)} relevant chunks)\\n\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"\\nâš ï¸ Error: {e}\\n\")\n",
    "    \n",
    "    return interactive_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40858ccc-5533-4ae5-8f67-de8fbd5e63bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_chatbot(\n",
    "    video_id: str = Config.VIDEO_ID,\n",
    "    use_multi_query: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Main setup function - initializes everything and returns chat functions\n",
    "    \n",
    "    Args:\n",
    "        video_id: YouTube video ID\n",
    "        use_multi_query: Whether to use multi-query retrieval (slower but better)\n",
    "        \n",
    "    Returns:\n",
    "        chat_function, interactive_chat_function, memory, metadata\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ğŸš€ Setting up YouTube Chatbot V2...\\n\")\n",
    "    \n",
    "    # Step 1: Fetch transcript\n",
    "    print(\"ğŸ“¥ Fetching transcript...\")\n",
    "    transcript, metadata = fetch_youtube_transcript(video_id)\n",
    "    print(f\"âœ… Fetched {metadata['length']:,} characters\\n\")\n",
    "    \n",
    "    # Step 2: Chunk transcript\n",
    "    print(\"âœ‚ï¸ Chunking transcript...\")\n",
    "    chunks = chunk_transcript(transcript, metadata)\n",
    "    print(f\"âœ… Created {len(chunks)} chunks\\n\")\n",
    "    \n",
    "    # Step 3: Create embeddings and vector store\n",
    "    print(\"ğŸ§® Creating embeddings and vector store...\")\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=Config.EMBEDDING_MODEL,\n",
    "        model_kwargs={'device': 'cpu'},\n",
    "        encode_kwargs={'normalize_embeddings': True}\n",
    "    )\n",
    "    vector_store = FAISS.from_documents(chunks, embeddings)\n",
    "    print(f\"âœ… Vector store created\\n\")\n",
    "    \n",
    "    # Step 4: Create BM25 index\n",
    "    print(\"ğŸ“š Creating BM25 index...\")\n",
    "    tokenized_docs = [doc.page_content.lower().split() for doc in chunks]\n",
    "    bm25_index = BM25Okapi(tokenized_docs)\n",
    "    print(f\"âœ… BM25 index created\\n\")\n",
    "    \n",
    "    # Step 5: Load reranker\n",
    "    print(\"ğŸ¯ Loading reranker...\")\n",
    "    reranker = CrossEncoder(Config.RERANKER_MODEL)\n",
    "    print(f\"âœ… Reranker loaded\\n\")\n",
    "    \n",
    "    # Step 6: Initialize LLM\n",
    "    print(\"ğŸ¤– Initializing LLM...\")\n",
    "    llm = HuggingFaceEndpoint(\n",
    "        repo_id=Config.LLM_MODEL,\n",
    "        huggingfacehub_api_token=Config.HF_TOKEN,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95\n",
    "    )\n",
    "    chat_model = ChatHuggingFace(llm=llm)\n",
    "    print(f\"âœ… LLM initialized\\n\")\n",
    "    \n",
    "    # Step 7: Create retrievers\n",
    "    print(\"ğŸ” Creating retrieval system...\")\n",
    "    hybrid_retriever = HybridRetriever(\n",
    "        vector_store=vector_store,\n",
    "        bm25_index=bm25_index,\n",
    "        chunks=chunks,\n",
    "        reranker=reranker\n",
    "    )\n",
    "    \n",
    "    if use_multi_query:\n",
    "        retriever = MultiQueryRetriever(\n",
    "            base_retriever=hybrid_retriever,\n",
    "            llm=chat_model\n",
    "        )\n",
    "        print(f\"âœ… Multi-query retrieval enabled\\n\")\n",
    "    else:\n",
    "        retriever = hybrid_retriever\n",
    "        print(f\"âœ… Standard hybrid retrieval enabled\\n\")\n",
    "    \n",
    "    # Step 8: Initialize memory\n",
    "    print(\"ğŸ’­ Initializing conversation memory...\")\n",
    "    memory = ConversationMemory()\n",
    "    print(f\"âœ… Memory initialized\\n\")\n",
    "    \n",
    "    # Step 9: Create chain\n",
    "    print(\"â›“ï¸ Building RAG chain...\")\n",
    "    chain = create_rag_chain(\n",
    "        retriever=retriever,\n",
    "        llm=chat_model,\n",
    "        memory=memory\n",
    "    )\n",
    "    print(f\"âœ… Chain created\\n\")\n",
    "    \n",
    "    # Step 10: Create chat functions\n",
    "    chat_function = create_chat_function(chain, memory)\n",
    "    interactive_chat_function = create_interactive_chat(\n",
    "        chat_function, \n",
    "        memory, \n",
    "        metadata['url']\n",
    "    )\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"âœ… SETUP COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nğŸ“Š System Summary:\")\n",
    "    print(f\"   â€¢ Video: {metadata['url']}\")\n",
    "    print(f\"   â€¢ Chunks: {len(chunks)}\")\n",
    "    print(f\"   â€¢ Retrieval: {'Multi-Query' if use_multi_query else 'Hybrid'}\")\n",
    "    print(f\"   â€¢ Reranking: Enabled\")\n",
    "    print(f\"   â€¢ Memory: Last {Config.MAX_HISTORY} interactions\")\n",
    "    print(\"\\nğŸ¯ Ready to chat!\\n\")\n",
    "    print(\"Usage:\")\n",
    "    print(\"  answer, contexts = chat_function('Your question here')\")\n",
    "    print(\"  interactive_chat_function()  # Start interactive chat\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    return chat_function, interactive_chat_function, memory, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ad5160e-30cf-40e8-8ccd-72ae8bc6039a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Setting up YouTube Chatbot V2...\n",
      "\n",
      "ğŸ“¥ Fetching transcript...\n",
      "âœ… Fetched 108,990 characters\n",
      "\n",
      "âœ‚ï¸ Chunking transcript...\n",
      "âœ… Created 144 chunks\n",
      "\n",
      "ğŸ§® Creating embeddings and vector store...\n",
      "âœ… Vector store created\n",
      "\n",
      "ğŸ“š Creating BM25 index...\n",
      "âœ… BM25 index created\n",
      "\n",
      "ğŸ¯ Loading reranker...\n",
      "âœ… Reranker loaded\n",
      "\n",
      "ğŸ¤– Initializing LLM...\n",
      "âœ… LLM initialized\n",
      "\n",
      "ğŸ” Creating retrieval system...\n",
      "âœ… Multi-query retrieval enabled\n",
      "\n",
      "ğŸ’­ Initializing conversation memory...\n",
      "âœ… Memory initialized\n",
      "\n",
      "â›“ï¸ Building RAG chain...\n",
      "âœ… Chain created\n",
      "\n",
      "================================================================================\n",
      "âœ… SETUP COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š System Summary:\n",
      "   â€¢ Video: https://www.youtube.com/watch?v=kCc8FmEb1nY\n",
      "   â€¢ Chunks: 144\n",
      "   â€¢ Retrieval: Multi-Query\n",
      "   â€¢ Reranking: Enabled\n",
      "   â€¢ Memory: Last 5 interactions\n",
      "\n",
      "ğŸ¯ Ready to chat!\n",
      "\n",
      "Usage:\n",
      "  answer, contexts = chat_function('Your question here')\n",
      "  interactive_chat_function()  # Start interactive chat\n",
      "================================================================================\n",
      "\n",
      "\n",
      "ğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ª\n",
      "TESTING SINGLE QUESTION\n",
      "ğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ªğŸ§ª\n",
      "\n",
      "\n",
      "================================================================================\n",
      "â“ Question: What is this video about?\n",
      "================================================================================\n",
      "\n",
      "ğŸ” Generated Queries:\n",
      "   1. What is this video about?\n",
      "   2. 1. What is the topic of this video?\n",
      "   3. 2. Can you describe the content of this video?\n",
      "\n",
      "ğŸ¤– Answer: This video is about the implementation of a decoder-only Transformer model for text generation in Python using Jupyter Notebook. The speaker explains that the model is missing some components, such as the encoder and the cross attention block, which are necessary for processing input text and allowing tokens to attend to each other. The video assumes some prior knowledge of calculus, statistics, and neural networks, particularly the concepts introduced in the speaker's previous videos on the same channel. The goal is to develop a language model using the Transformer neural network.\n",
      "\n",
      "ğŸ’¡ (Retrieved 4 relevant chunks)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "â“ Question: Can you explain it in more detail?\n",
      "================================================================================\n",
      "\n",
      "ğŸ” Generated Queries:\n",
      "   1. Can you explain it in more detail?\n",
      "   2. 1. Could you please provide a more comprehensive explanation?\n",
      "   3. 2. Can you give a more detailed account of the subject?\n",
      "\n",
      "ğŸ¤– Answer: The context from the video discusses the concept of attention in the Transformer model for text generation. Attention is described as a communication mechanism between nodes in a graph, where each node aggregates information from other nodes it is connected to via a weighted sum. The structure of the directed graph has each node pointing to all previous nodes, allowing a token to receive information only from tokens that perceive it. The video also mentions using softmax and layer norms to improve the model's performance. The speaker has a complete decoder-only Transformer model according to the original paper and plans to scale it up for better performance.\n",
      "\n",
      "ğŸ’¡ (Retrieved 4 relevant chunks)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "â“ Question: What are the main concepts?\n",
      "================================================================================\n",
      "\n",
      "ğŸ” Generated Queries:\n",
      "   1. What are the main concepts?\n",
      "   2. 1. Could you please explain the key ideas or themes?\n",
      "   3. 2. What are the fundamental principles or topics?\n",
      "\n",
      "ğŸ¤– Answer: The main concepts of the video are:\n",
      "\n",
      "1. The implementation of a single head of self-attention mechanism in the Transformer model for text generation.\n",
      "2. The use of skip connections (or residual connections) to improve the optimizability of deep neural networks.\n",
      "3. The distinction between self-attention and cross-attention, where self-attention is used when nodes only want to look at each other, and cross-attention is used when there's a separate source of nodes to pull information from.\n",
      "4. The introduction of multi-head attention, which applies multiple attention mechanisms in parallel and concatenates their results.\n",
      "\n",
      "These concepts help deep neural networks, such as encoder-decoder Transformers, remain optimizable and perform well, especially as they grow in depth.\n",
      "\n",
      "ğŸ’¡ (Retrieved 4 relevant chunks)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬\n",
      "STARTING INTERACTIVE CHAT\n",
      "ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬ğŸ’¬\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Example usage of the chatbot\n",
    "    \"\"\"\n",
    "    \n",
    "    # Setup chatbot (now always uses multi-query)\n",
    "    chat_function, interactive_chat, memory, metadata = setup_chatbot()\n",
    "    \n",
    "    # Example 1: Single question\n",
    "    print(\"\\n\" + \"ğŸ§ª\"*40)\n",
    "    print(\"TESTING SINGLE QUESTION\")\n",
    "    print(\"ğŸ§ª\"*40 + \"\\n\")\n",
    "    \n",
    "    answer, contexts, queries = chat_function(\"What is this video about?\")\n",
    "    \n",
    "    # Example 2: Follow-up question (uses memory)\n",
    "    answer, contexts, queries = chat_function(\"Can you explain it in more detail?\")\n",
    "    \n",
    "    # Example 3: Another question\n",
    "    answer, contexts, queries = chat_function(\"What are the main concepts?\")\n",
    "    \n",
    "    # Example 4: Start interactive chat\n",
    "    print(\"\\n\" + \"ğŸ’¬\"*40)\n",
    "    print(\"STARTING INTERACTIVE CHAT\")\n",
    "    print(\"ğŸ’¬\"*40 + \"\\n\")\n",
    "    \n",
    "    # Uncomment to start interactive mode:\n",
    "    # interactive_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0eb4cef1-4a50-4f2e-8948-c22652717be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ’¬ YOUTUBE VIDEO CHATBOT - Interactive Mode\n",
      "================================================================================\n",
      "\n",
      "ğŸ”¹ Video: https://www.youtube.com/watch?v=kCc8FmEb1nY\n",
      "\n",
      "ğŸ® Commands:\n",
      "   - Type your question to chat\n",
      "   - 'quit' or 'exit': End chat\n",
      "   - 'clear': Clear conversation history\n",
      "   - 'history': Show conversation history\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  what is happening in this video?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Generated Queries:\n",
      "   1. what is happening in this video?\n",
      "   2. 1. Could you describe the events taking place in this video?\n",
      "   3. 2. What actions are depicted in this video?\n",
      "\n",
      "ğŸ¤– Bot: In this video, the speaker is implementing a decoder-only Transformer model for text generation using Python and Jupyter Notebook. They are focusing on the self-attention mechanism and cross-attention in the model. The encoder reads French text, and all tokens are allowed to communicate with each other during encoding. In the decoder, there is an additional connection to the encoder outputs through a cross attention, where keys and values come from the encoder outputs. The speaker also discusses the use of embedding tables and logits in predicting the next character based on token identity. They assume prior knowledge of calculus, statistics, and neural networks, particularly the concepts introduced in the speaker's previous videos on the same channel. The goal is to develop a language model using the Transformer neural network.\n",
      "\n",
      "ğŸ’¡ (Retrieved 4 relevant chunks)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‘‹ Goodbye! Thanks for chatting!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "interactive_chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
